{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 Pandas Exercises for Data Analysis\n",
    "\n",
    "*by Selva Prabhakaran*\n",
    "\n",
    "From the website: https://www.machinelearningplus.com/python/101-pandas-exercises-python/\n",
    "\n",
    "*101 python pandas exercises are designed to challenge your logical muscle and to help internalize data manipulation with pythonâ€™s favorite package for data analysis.*\n",
    "*The questions are of 3 levels of difficulties with L1 being the easiest to L3 being the hardest.*\n",
    "\n",
    "**NOTE**: Again, its 75 not 100, but the exercises are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (L1) **Import `pandas` and check the version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (L1) **Create a `pandas` series from each of the items below: a list, numpy and a dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inputs\n",
    "\n",
    "mylist = list('abcedfghijklmnopqrstuvwxyz')\n",
    "myarr = np.arange(26)\n",
    "mydict = dict(zip(mylist, myarr))\n",
    "\n",
    "for data in [mylist, myarr, mydict]:\n",
    "    print(pd.Series(data).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (L1) **Convert the series `ser` into a dataframe with its index as another column on the dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "mylist = list('abcedfghijklmnopqrstuvwxyz')\n",
    "myarr = np.arange(26)\n",
    "mydict = dict(zip(mylist, myarr))\n",
    "ser = pd.Series(mydict)\n",
    "\n",
    "ser.to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (L1) **Combine `ser1` and `ser2` to form a dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "ser1 = pd.Series(list('abcedfghijklmnopqrstuvwxyz'))\n",
    "ser2 = pd.Series(np.arange(26))\n",
    "\n",
    "np.all(\n",
    "    pd.DataFrame([ser1, ser2]).T  # Stacking horizontally and rotating by 90 deg\n",
    "    == pd.concat([ser1, ser2], axis=1)  # Stacking rotated series\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (L1) **Give a name to the series `ser` calling it `my_series`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(list('abcedfghijklmnopqrstuvwxyz'))\n",
    "\n",
    "ser.name = \"my_series\"\n",
    "\n",
    "ser.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. (L2) **From `ser1` remove items present in `ser2`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "ser1 = pd.Series([1, 2, 3, 4, 5])\n",
    "ser2 = pd.Series([4, 5, 6, 7, 8])\n",
    "\n",
    "# With numpy\n",
    "with_numpy = np.setdiff1d(ser1, ser2)\n",
    "\n",
    "# With pandas\n",
    "with_pandas = ser1[~ser1.isin(ser2)]\n",
    "\n",
    "print(with_numpy)  # Results in ndarray\n",
    "print(with_pandas)  # Results in Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. (L2) **Get the items not common to both series `ser1` and `ser2`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser1 = pd.Series([1, 2, 3, 4, 5])\n",
    "ser2 = pd.Series([4, 5, 6, 7, 8])\n",
    "\n",
    "union = np.union1d(ser1, ser2)\n",
    "intersection = np.intersect1d(ser1, ser2)\n",
    "\n",
    "print(union, intersection)\n",
    "union[~pd.Series(union).isin(intersection)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. (L2) **Compute the minimum, 25th percentile, median, 75th, and maximum of `ser`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(np.random.normal(10, 5, 25))\n",
    "\n",
    "for method in [pd.Series.max, pd.Series.min, pd.Series.median]:\n",
    "    print(method(ser))\n",
    "print(ser.quantile(0.25))\n",
    "print(ser.quantile(0.75))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. (L2) **Calcualte the frequency counts of each unique item in `ser`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(np.take(list('abcdefgh'), np.random.randint(8, size=30)))\n",
    "\n",
    "# Through numpy\n",
    "with_numpy = np.unique(ser.to_numpy(), return_counts=True)\n",
    "\n",
    "# Through pandas\n",
    "with_pandas = ser.value_counts()\n",
    "\n",
    "print(with_numpy)\n",
    "print(with_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. (L2) **From `ser`, keep the top 2 most frequent items as it is and replace everything else as `Other`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "np.random.RandomState(100)\n",
    "ser = pd.Series(np.random.randint(1, 5, [12]))\n",
    "\n",
    "\n",
    "ser[~ser.isin(ser.value_counts()[:2].index.to_series())] = 'Other'\n",
    "print(ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. (L2) **Bin the series `ser` into 10 equal deciles and replace the values with the bin name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(np.random.random(20))\n",
    "\n",
    "with_cut = pd.cut(\n",
    "    ser,\n",
    "    bins=np.percentile(ser, np.arange(0, 110, 10)),\n",
    "    labels=['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "with_qcut = pd.qcut(\n",
    "    ser,\n",
    "    q=np.arange(0, 1.1, .1),\n",
    "    labels=['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th']\n",
    ")\n",
    "\n",
    "np.all(with_cut == with_qcut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. (L1) **Reshape the series `ser` into a dataframe with 7 rows and 5 columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(np.random.randint(1, 10, 35))\n",
    "\n",
    "pd.DataFrame(\n",
    "    ser.to_numpy().reshape((7,5))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. (L2)  **Find the positions of numbers that are multiples of 3 from `ser`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(np.random.randint(1, 10, 7))\n",
    "\n",
    "print(ser)\n",
    "np.argwhere(ser.to_numpy() % 3 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. (L1) **From `ser`, extract the items at positions in list `pos`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(list('abcdefghijklmnopqrstuvwxyz'))\n",
    "pos = [0, 4, 8, 14, 20]\n",
    "\n",
    "np.all(ser[pos] == ser.take(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. (L1) **Stack two series vertically and horizontally.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser1 = pd.Series(range(5))\n",
    "ser2 = pd.Series(list('abcde'))\n",
    "\n",
    "horizontally = pd.concat([ser1, ser2], axis=0)\n",
    "vertically = pd.concat([ser1, ser2], axis=1)\n",
    "print(horizontally, vertically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. (L2) **Get the positions of items of `ser2` in `ser1` as a list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser1 = pd.Series([10, 9, 6, 5, 3, 1, 12, 8, 13])\n",
    "ser2 = pd.Series([1, 3, 10, 13])\n",
    "\n",
    "np.argwhere(\n",
    "    ser1.isin(ser2).to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. (L2) **Compute the mean squared error of `truth` and `pred`.**\n",
    "\n",
    "**NOTE**: This question means that we need to calculate the mean squared error between the two series, using the formula:\n",
    "\n",
    "$$MSE = \\dfrac{1}{n} * \\sum \\left(truth - pred\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "truth = pd.Series(range(10))\n",
    "pred = pd.Series(range(10)) + np.random.random(10)\n",
    "\n",
    "np.mean(\n",
    "    (truth-pred)**2  # Squares of errors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. (L2) **Convert the first character of each element in `ser` to uppercase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(['how', 'to', 'kick', 'ass?'])\n",
    "\n",
    "ser.apply(str.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. (L2) **Calculate the number of characters in each word in `ser`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(['how', 'to', 'kick', 'ass?'])\n",
    "\n",
    "ser.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. (L2) **Compute the difference of differences between consecutive numbers in `ser`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series([1, 3, 6, 10, 15, 21, 27, 35])\n",
    "\n",
    "ser.diff().diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. (L2) **Convert a series of date-strings to a timeseries**\n",
    "\n",
    "Desired output:\n",
    "\n",
    "```\n",
    "0   2010-01-01 00:00:00\n",
    "1   2011-02-02 00:00:00\n",
    "2   2012-03-03 00:00:00\n",
    "3   2013-04-04 00:00:00\n",
    "4   2014-05-05 00:00:00\n",
    "5   2015-06-06 12:20:00\n",
    "dtype: datetime64[ns]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(['01 Jan 2010', '02-02-2011', '20120303', '2013/04/04', '2014-05-05', '2015-06-06T12:20'])\n",
    "\n",
    "ser.astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. (L2) **Get the day of month, week number, day of year and day of week from `ser`.**\n",
    "\n",
    "Desired output:\n",
    "\n",
    "```\n",
    "Date:  [1, 2, 3, 4, 5, 6]\n",
    "Week number:  [53, 5, 9, 14, 19, 23]\n",
    "Day num of year:  [1, 33, 63, 94, 125, 157]\n",
    "Day of week:  ['Friday', 'Wednesday', 'Saturday', 'Thursday', 'Monday', 'Saturday']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(['01 Jan 2010', '02-02-2011', '20120303', '2013/04/04', '2014-05-05', '2015-06-06T12:20'])\n",
    "\n",
    "# Manual\n",
    "\n",
    "dates = []\n",
    "week_numbers = []\n",
    "day_numbers = []\n",
    "weekdays = []\n",
    "weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Friday']\n",
    "\n",
    "for index, date in enumerate(ser.astype('datetime64[ns]')):\n",
    "    dates.append(index)\n",
    "    week_numbers.append(date.week)\n",
    "    day_numbers.append(date.day_of_year)\n",
    "    weekdays.append(weekday_names[date.weekday()])\n",
    "\n",
    "print('Manual')\n",
    "print(dates)\n",
    "print(week_numbers)\n",
    "print(day_numbers)\n",
    "print(weekdays)\n",
    "\n",
    "# With 'dt' property (incomplete)\n",
    "\n",
    "import dateutil\n",
    "\n",
    "timeseries = ser.map(dateutil.parser.parse)\n",
    "\n",
    "print('\\nThrough API')\n",
    "print(timeseries.dt.day.to_list())\n",
    "#print(timeseries.dt.weekofyear.to_list()) Doesn't work anymore, maybe API changed!\n",
    "print(timeseries.dt.day_of_year.to_list())\n",
    "print(timeseries.dt.weekday.map(lambda weekday_number: weekday_names[weekday_number]).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. (L2) **Change `ser` to dates that start with 4th of the respective months.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(['Jan 2010', 'Feb 2011', 'Mar 2012'])\n",
    "\n",
    "import datetime\n",
    "def shift_day(day_string: str) -> datetime.datetime:\n",
    "    return dateutil.parser.parse(f'4th {day_string}')\n",
    "\n",
    "pd.Series(\n",
    "    np.vectorize(shift_day)(ser)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. (L3) **From `ser`, extract words that contain atleast 2 vowels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(['Apple', 'Orange', 'Plan', 'Python', 'Money'])\n",
    "\n",
    "def count_vowels(word: str) -> int:\n",
    "    return len([\n",
    "        letter\n",
    "        for letter in np.array([*word.lower()])\n",
    "        if letter in ['a', 'e', 'i', 'o', 'u']\n",
    "    ])\n",
    "\n",
    "ser[\n",
    "    ser.map(count_vowels) >= 2  # This serves as mask for the ser Series\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. (L3) **Extract the valid `emails` from the series emails. The regex pattern for valid emails is provided as reference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "emails = pd.Series(['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com'])\n",
    "pattern ='[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,4}'\n",
    "\n",
    "import re\n",
    "\n",
    "emails[\n",
    "    emails.map(\n",
    "        lambda input_email: re.compile(pattern).match(input_email)\n",
    "    ).values != None\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. (L2) **Compute the mean of `weights` of each `fruit`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "fruit = pd.Series(np.random.choice(['apple', 'banana', 'carrot'], 10))\n",
    "weights = pd.Series(np.linspace(1, 10, 10))\n",
    "\n",
    "fruits_dataframe = pd.DataFrame(\n",
    "    np.vstack([fruit, weights])\n",
    ").T\n",
    "fruits_dataframe.columns = [\"fruit\", \"weight\"]\n",
    "fruits_dataframe.groupby('fruit').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. (L2) **Compute the euclidean distance between series (points) p and q, without using a packaged formula.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "p = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "q = pd.Series([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])\n",
    "\n",
    "via_dataframe = np.sqrt(\n",
    "    pd.DataFrame([p,q]).T.apply(lambda row: (row[1]-row[0])**2, axis=1).sum(axis=0)\n",
    ")\n",
    "via_norm = np.sqrt(np.sum((q-p)**2))\n",
    "\n",
    "np.isclose(via_dataframe, via_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. (L3) **Get the positions of peaks (values surrounded by smaller values on both sides) in `ser`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series([2, 10, 3, 4, 9, 10, 2, 7, 3])\n",
    "\n",
    "# From NumPy exercises\n",
    "a_diffs = np.sign(np.diff(np.hstack([ser[0], ser])))\n",
    "[\n",
    "    diff_index\n",
    "    for diff_index in range(len(a_diffs[1:]))\n",
    "    if a_diffs[diff_index] == 1 and a_diffs[diff_index+1] == -1  # Signs of differences: (+1) / [MAX] \\ (-1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. (L3) **Replace missing spaces in the string `my_str` with the least frequent character.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "my_str = 'dbc deb abed gade'\n",
    "\n",
    "uniques, counts = np.unique(\n",
    "    pd.Series([*my_str]),\n",
    "    return_counts=True\n",
    ")\n",
    "least_frequent_character = uniques[np.argmin(counts)]\n",
    "\n",
    "my_str.replace(' ', least_frequent_character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. (L3) **Create a TimeSeries starting `2000-01-01` and 10 weekends (saturdays) after that having random numbers as values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'date': pd.date_range('2020-01-01', periods=10, freq='7D'),\n",
    "    'number': np.random.randint(low=0, high=100, size=10)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. (L2) **`ser` has missing dates and values. Make all missing dates appear and fill up with value from previous date.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series([1,10,3,np.nan], index=pd.to_datetime(['2000-01-01', '2000-01-03', '2000-01-06', '2000-01-08']))\n",
    "\n",
    "'''\n",
    "From docs:\n",
    "\n",
    "Series.resample: method for frequency conversion and resampling of time series\n",
    "\n",
    "Here: instead of sampling at random days, we shift the frequency to ONE DAY and then fill missing values with ffill\n",
    "'''\n",
    "ser.resample('D').ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. (L3) **Compute autocorrelations for the first 10 lags of `ser`. Find out which lag has the largest correlation.**\n",
    "\n",
    "Desired output:\n",
    "\n",
    "```python\n",
    "# values will change due to randomness\n",
    "[0.29999999999999999, -0.11, -0.17000000000000001, 0.46000000000000002, 0.28000000000000003, -0.040000000000000001, -0.37, 0.41999999999999998, 0.47999999999999998, 0.17999999999999999]\n",
    "Lag having highest correlation:  9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "ser = pd.Series(np.arange(20) + np.random.normal(1, 10, 20))\n",
    "\n",
    "autocorrelations = pd.Series([\n",
    "    ser.autocorr(lag)\n",
    "    for lag in range(1,11)\n",
    "])\n",
    "\n",
    "print(autocorrelations.to_list())\n",
    "print(autocorrelations.copy().abs().argmax() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33. (L2) **Import every 50th row of `boston` dataset as a dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "BOSTON_URL = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
    "\n",
    "pd.DataFrame([\n",
    "    chunk.iloc[0,:]  # This takes first line\n",
    "    for chunk in pd.read_csv(\n",
    "        BOSTON_URL,\n",
    "        chunksize=50  # Of every chunk that is 50 rows long\n",
    "    )\n",
    "]).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34. (L2) **Import the boston housing dataset, but while importing change the `medv` (median house value) column so that values < 25 becomes `Low` and > 25 becomes `High`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(row: np.generic):\n",
    "    row['medv'] = 'Low' if row['medv'] < 25 else 'High'\n",
    "    return row\n",
    "\n",
    "via_apply = pd.read_csv(BOSTON_URL).apply(categorize, axis=1)\n",
    "via_converters = pd.read_csv(BOSTON_URL, converters={\n",
    "    'medv': lambda median_value: 'Low' if float(median_value) < 25 else 'High'\n",
    "})\n",
    "np.all(via_apply == via_converters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. (L3) **Create a dataframe with rows as strides from a given series**\n",
    "\n",
    "Desired output:\n",
    "\n",
    "```python\n",
    "array([[ 0,  1,  2,  3],\n",
    "       [ 2,  3,  4,  5],\n",
    "       [ 4,  5,  6,  7],\n",
    "       [ 6,  7,  8,  9],\n",
    "       [ 8,  9, 10, 11],\n",
    "       [10, 11, 12, 13]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "L = pd.Series(range(15))\n",
    "\n",
    "window_width = 4\n",
    "stride_jump = 2\n",
    "\n",
    "pd.DataFrame(\n",
    "np.array(\n",
    "    [\n",
    "        L[stride_jump*stride:(stride_jump*stride+window_width)]\n",
    "        for stride in range(int((len(L)-window_width)/stride_jump) + 1)\n",
    "    ]\n",
    "    ).reshape(\n",
    "        (-1, window_width)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "36. (L1) **Import `crim` and `medv` columns of the BostonHousing dataset as a dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(BOSTON_URL, usecols=['crim', 'medv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "37. (L2) **Get the number of rows, columns, datatype and summary statistics of each column of the `Cars93` dataset. Also get the numpy array and list equivalent of the dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "CARS93_URL = \"https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv\"\n",
    "cars = pd.read_csv(CARS93_URL)\n",
    "\n",
    "print(\n",
    "    cars.info(),\n",
    "    '\\n',\n",
    "    cars.describe(),\n",
    "    '\\n',\n",
    "    cars.to_numpy(),\n",
    "    '\\n',\n",
    "    cars.to_numpy().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38. (L1) **Which manufacturer, model and type has the highest Price? What is the row and column number of the cell with the highest Price value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "cars = pd.read_csv(CARS93_URL)\n",
    "\n",
    "max_price_car = cars.loc[\n",
    "    np.argmax(  # Row of max price\n",
    "        cars['Price']\n",
    "    ), \n",
    "    ['Manufacturer', 'Model', 'Type']  # Which columns\n",
    "]\n",
    "print(max_price_car)\n",
    "np.where(  # Locate i,j of max Price cell\n",
    "    cars.values == np.max(cars['Price'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "39. (L2) **Rename the column `Type` as `CarType` in df and replace the `.` in column names with `_`.**\n",
    "\n",
    "Desired output:\n",
    "\n",
    "```python\n",
    " print(df.columns)\n",
    "#> Index(['Manufacturer', 'Model', 'CarType', 'Min_Price', 'Price', 'Max_Price',\n",
    "#>        'MPG_city', 'MPG_highway', 'AirBags', 'DriveTrain', 'Cylinders',\n",
    "#>        'EngineSize', 'Horsepower', 'RPM', 'Rev_per_mile', 'Man_trans_avail',\n",
    "#>        'Fuel_tank_capacity', 'Passengers', 'Length', 'Wheelbase', 'Width',\n",
    "#>        'Turn_circle', 'Rear_seat_room', 'Luggage_room', 'Weight', 'Origin',\n",
    "#>        'Make'],\n",
    "#>       dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "cars = pd.read_csv(CARS93_URL)\n",
    "\n",
    "cars.columns = [\n",
    "    column.replace('.', '_')\n",
    "    for column in cars.columns\n",
    "]\n",
    "cars.rename(columns={'Type': 'CarType'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40. (L1) **Check if `df` has any missing values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "cars = pd.read_csv(CARS93_URL)\n",
    "\n",
    "np.any(\n",
    "    cars.isna()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41. (L2) **Count the number of missing values in each column of `df`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "cars = pd.read_csv(CARS93_URL)\n",
    "\n",
    "nans = cars.isna().astype(int).sum(axis=0)\n",
    "nans[\n",
    "    nans == nans.max()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42. (L2) **Replace missing values in `Min.Price` and `Max.Price` columns with their respective mean.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "cars = pd.read_csv(CARS93_URL)\n",
    "\n",
    "for column in ['Min.Price', 'Max.Price']:\n",
    "    cars[column].fillna(\n",
    "        cars[column].mean().round(1),\n",
    "        inplace=True\n",
    "    )\n",
    "cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "43. (L3) **In `df`, use apply method to replace the missing values in `Min.Price` with the columnâ€™s mean and those in `Max.Price` with the columnâ€™s median.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "\n",
    "def filler(column: pd.Series):  # On deafult, apply happens PER COLUMN\n",
    "    if reducer := {\n",
    "        'Min.Price': np.nanmean,  # Drops NaN\n",
    "        'Max.Price': np.nanmedian,  # Ditto\n",
    "    }.get(column.name):\n",
    "        return column.fillna(value=reducer(column).round(1))\n",
    "    return column\n",
    "\n",
    "df.apply(filler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "44. (L2) **Get the first column (a) in `df` as a dataframe (rather than as a series).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "df = pd.DataFrame(np.arange(20).reshape(-1, 5), columns=list('abcde'))\n",
    "\n",
    "np.all(\n",
    "    pd.DataFrame(df[  # Brute-force\n",
    "        df.columns[0]\n",
    "    ]) == df[[df.columns[0]]]  # Shorthand\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45. (L3) Actually 3 questions.\n",
    "\n",
    "* **In `df`, interchange columns `a` and `c`.**\n",
    "* **Create a generic function to interchange two columns, without hardcoding column names.**\n",
    "* **Sort the columns in reverse alphabetical order, that is column `e` first through column `a` last.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "df = pd.DataFrame(np.arange(20).reshape(-1, 5), columns=list('abcde'))\n",
    "\n",
    "# Swapping function\n",
    "def swap_axes(dataframe: pd.DataFrame, axis_1: str, axis_2: str) -> None:\n",
    "    if axis_1 == axis_2:\n",
    "        return\n",
    "    dataframe[[axis_1, axis_2]] = df[[axis_2, axis_1]]\n",
    "    dataframe.rename(mapper={axis_1: axis_2, axis_2: axis_1}, inplace=True, axis=1)\n",
    "\n",
    "# Reversing with the function\n",
    "for mapping_index in range(len(df.columns)):\n",
    "    axis_1 = df.columns[mapping_index] \n",
    "    axis_2 = sorted(df.columns, reverse=True)[mapping_index]\n",
    "    swap_axes(df, axis_1=axis_1, axis_2=axis_2)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Sorting alphabetically, descending\n",
    "df.sort_index(axis=1, ascending=False, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46. (L2) **Change the pandas display settings on printing the dataframe `df` it shows a maximum of 10 rows and 10 columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47. (L2) **Suppress scientific notations like `e-03` in df and print upto 4 numbers after decimal.**\n",
    "\n",
    "Desired output:\n",
    "\n",
    "```python\n",
    "#>    random\n",
    "#> 0  0.0035\n",
    "#> 1  0.0000\n",
    "#> 2  0.0747\n",
    "#> 3  0.0000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "df = pd.DataFrame(np.random.random(4)**10, columns=['random'])\n",
    "\n",
    "pd.set_option('display.precision', 4)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48. (L2) **Format the values in column `random` of `df` as percentages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "df = pd.DataFrame(np.random.random(4), columns=['random'])\n",
    "\n",
    "df.style.format({\n",
    "    'random': lambda value: f'{round((value * 100), 2)}%'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49. (L1) **From `df`, filter the `Manufacturer`, `Model` and `Type` for every 20th row starting from 1st (row 0).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "\n",
    "df[['Manufacturer', 'Model', 'Type']].iloc[::20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50. (L2) **In `df`, Replace NaNs with â€˜missingâ€™ in columns 'Manufacturer', 'Model' and 'Type' and create a index as a combination of these three columns and check if the index is a primary key.**\n",
    "\n",
    "Desired output:\n",
    "\n",
    "```python\n",
    "                       Manufacturer    Model     Type  Min.Price  Max.Price\n",
    "Acura_Integra_Small           Acura  Integra    Small       12.9       18.8\n",
    "missing_Legend_Midsize      missing   Legend  Midsize       29.2       38.7\n",
    "Audi_90_Compact                Audi       90  Compact       25.9       32.3\n",
    "Audi_100_Midsize               Audi      100  Midsize        NaN       44.6\n",
    "BMW_535i_Midsize                BMW     535i  Midsize        NaN        NaN\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv', usecols=[0,1,2,3,5])\n",
    "\n",
    "df[['Manufacturer', 'Model', 'Type']] = df[['Manufacturer', 'Model', 'Type']].fillna(value='missing')\n",
    "df.set_index(\n",
    "    df['Manufacturer'] + '_' + df['Model'] + '_' + df['Type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
